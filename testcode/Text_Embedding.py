import sys
import os
sys.path.append(os.path.abspath(os.path.dirname(__file__)))
from langchain.text_splitter import SpacyTextSplitter
from langchain_openai.embeddings import OpenAIEmbeddings
from langchain_pinecone import PineconeVectorStore
import spacy
from pinecone import Pinecone
from langchain.schema import Document
from dotenv import load_dotenv

load_dotenv()
print(os.getenv("OPENAI_API_KEY"))  # Noneì´ë©´ .env íŒŒì¼ì„ ëª» ì½ê³  ìˆëŠ” ê²ƒ

# ğŸ”¹ ì¼ë°˜ í…ìŠ¤íŠ¸ ì…ë ¥
your_text = """
ì•„ì£¼ëŒ€í•™êµ ë‚´ì—ëŠ” ì„±í˜¸ê´€, íŒ”ë‹¬ê´€, ì¼ì‹ ê´€, ë‹¤ì‚°ê´€ì— í¸ì˜ì ì´ ìˆì–´.
"""

# ğŸ”¹ ì¼ë°˜ í…ìŠ¤íŠ¸ë¥¼ Document ê°ì²´ë¡œ ë³€í™˜
documents = [Document(page_content=your_text, metadata={"source": "manual_input"})]

print(f"âœ… ì¼ë°˜ í…ìŠ¤íŠ¸ 1ê°œë¥¼ ë¬¸ì„œë¡œ ë³€í™˜ ì™„ë£Œ!")

# ğŸ”¹ Spacy ëª¨ë¸ ë¡œë“œ ë° ë¬¸ì„œ ë¶„í• 
nlp = spacy.load("ko_core_news_sm")
nlp.max_length = 2_000_000

text_splitter = SpacyTextSplitter(
    chunk_size=300,
    pipeline="ko_core_news_sm",
    max_length=2000000,
    separator="\n\n"
)
splitted_documents = text_splitter.split_documents(documents)

print(f"âœ… ì¼ë°˜ í…ìŠ¤íŠ¸ê°€ ì´ {len(splitted_documents)} ê°œì˜ ì²­í¬ë¡œ ë¶„í• ë˜ì—ˆìŠµë‹ˆë‹¤.")

# ğŸ”¹ OpenAI Embeddings ì´ˆê¸°í™”
embeddings = OpenAIEmbeddings(
    model="text-embedding-ada-002",
    openai_api_key=os.getenv("OPENAI_API_KEY")
)

# ğŸ”¹ Pinecone ì´ˆê¸°í™”
pinecone_client = Pinecone(
    api_key=os.getenv("PINECONE_API_KEY")
)

# ğŸ”¹ Pinecone VectorStore ì´ˆê¸°í™” ë° ë¬¸ì„œ ì¶”ê°€
index_name = "ajou-data"
vectorstore = PineconeVectorStore(
    embedding=embeddings,
    index_name=index_name
)

vectorstore.add_documents(splitted_documents)  # ğŸ”¹ ëª¨ë“  ë¬¸ì„œ í•œ ë²ˆì— ì—…ë¡œë“œ

print("âœ… ì¼ë°˜ í…ìŠ¤íŠ¸ë¥¼ Pinecone ë°ì´í„°ë² ì´ìŠ¤ì— ì €ì¥ ì™„ë£Œ!")